{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a7e57ca",
   "metadata": {},
   "source": [
    "#   Captcha Training Model  And we got the accuracy of above 92 %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4fe757",
   "metadata": {},
   "source": [
    "## This is 6 letter captcha for prediction \n",
    "## i am not providing any data set or any captcha due to some security reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8066c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some important library\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import selenium\n",
    "import urllib.request\n",
    "import pytesseract\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from PIL import ImageFilter\n",
    "import cv2\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from matplotlib import pyplot as plt # graphics\n",
    "\n",
    "\n",
    "def prepare_image(img):\n",
    "    \"\"\"Transform image to greyscale and blur it\"\"\"\n",
    "    img = img.filter(ImageFilter.SMOOTH_MORE)\n",
    "    img = img.filter(ImageFilter.SMOOTH_MORE)\n",
    "    if 'L' != img.mode:\n",
    "        img = img.convert('L')\n",
    "    return img\n",
    "\n",
    "def remove_noise(img, pass_factor):\n",
    "    for column in range(img.size[0]):\n",
    "        for line in range(img.size[1]):\n",
    "            value = remove_noise_by_pixel(img, column, line, pass_factor)\n",
    "            img.putpixel((column, line), value)\n",
    "    return img\n",
    "\n",
    "def remove_noise_by_pixel(img, column, line, pass_factor):\n",
    "    if img.getpixel((column, line)) < pass_factor:\n",
    "        return (0)\n",
    "    return (255)\n",
    "pass_factor=186\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96946c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "#  we can also use glob to get all files and easy to get using glob\n",
    "filelist = []\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk(r'C:\\Users\\ADMIN\\Desktop\\Python\\Code\\ML\\Captcha Model\\Captcha'):\n",
    "    for filename in filenames:\n",
    "        #print(os.path.join(dirname, filename))\n",
    "        filelist.append(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c35df500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\ADMIN\\\\Desktop\\\\Python\\\\Code\\\\ML\\\\Captcha Model\\\\Captcha\\\\009r1z.png']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df412674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating two empty list for appending images and labels data\n",
    "labels = []\n",
    "images = []\n",
    "#Importing Images from File-List\n",
    "for image_path in filelist[:1500]:\n",
    "    image = Image.open(image_path).filter(ImageFilter.SHARPEN)\n",
    "    image = image.convert('L') #convert image to black and white\n",
    "    pass_factor = 186\n",
    "    image = prepare_image(image) # applying function for preparing image\n",
    "    image=remove_noise(image,pass_factor) \n",
    "    image = np.array(image)  # convert image to array\n",
    "    image = image/255 #Normalization\n",
    "    \n",
    "    image_label = image_path.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "    \n",
    "    images.append(image)\n",
    "    labels.append(image_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b988187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 80, 200)\n"
     ]
    }
   ],
   "source": [
    "#Stacking Images to a Numpy Array\n",
    "images = np.stack(images)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aea75b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-Hot Encoding List of 6 character strings\n",
    "def one_hot_encode(characters, char_pool):\n",
    "    char_to_index = {char: i for i, char in enumerate(char_pool)}\n",
    "    num_classes = len(char_pool)\n",
    "    encoding_size = len(characters[0])\n",
    "    \n",
    "    one_hot_encoded = np.zeros((len(characters), encoding_size, num_classes), dtype=int)\n",
    "\n",
    "    for i, word in enumerate(characters):\n",
    "        for j, char in enumerate(word):\n",
    "            index = char_to_index[char]\n",
    "            one_hot_encoded[i, j, index] = 1\n",
    "\n",
    "    return one_hot_encoded\n",
    "\n",
    "# Character pool: digits 0-9 and lowercase letters a-z\n",
    "char_pool = '0123456789abcdefghijklmnopqrstuvwxyz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ac8c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding Labels\n",
    "encoded_labels = one_hot_encode(labels, char_pool)\n",
    "encoded_labels = np.array(encoded_labels)\n",
    "# encoded_labels.shape, encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d90add56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 80, 200), (1500, 6, 36))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shuffling Array\n",
    "indices = np.random.permutation(len(labels))\n",
    "\n",
    "images = images[indices]\n",
    "encoded_labels = encoded_labels[indices]\n",
    "\n",
    "images.shape, encoded_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b43dce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use this for split\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X = encoded_labels\n",
    "# y = images\n",
    "# train_labels, val_labels, train_images, val_images = train_test_split(X,y,test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "59fd323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-Validation Split\n",
    "split_index = int(len(labels) * 0.9)\n",
    "\n",
    "train_labels = encoded_labels[:split_index]\n",
    "val_labels = encoded_labels[split_index:]\n",
    "\n",
    "train_images = images[:split_index]\n",
    "val_images = images[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4363e1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 80, 200)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "103be437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6ec2d213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 78, 198, 32)       320       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 78, 198, 32)       0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 78, 198, 32)       128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 39, 99, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 39, 99, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 37, 97, 32)        9248      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 37, 97, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 37, 97, 32)        128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 35, 95, 32)        9248      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 35, 95, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 35, 95, 32)        128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 17, 47, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 17, 47, 32)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 15, 45, 64)        18496     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 15, 45, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 15, 45, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 7, 22, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 7, 22, 64)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 7, 22, 64)         4160      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 7, 22, 64)         0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 7, 22, 64)         256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 3, 11, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 3, 11, 64)         0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 3, 11, 128)        8320      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 3, 11, 128)        0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 3, 11, 128)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4224)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              8652800   \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 2048)              0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 2048)              8192      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 1024)              0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 1024)              4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 216)               221400    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 6, 36)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6, 36)             1332      \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 6, 36)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11036684 (42.10 MB)\n",
      "Trainable params: 11030092 (42.08 MB)\n",
      "Non-trainable params: 6592 (25.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Activation, BatchNormalization, MaxPooling2D, Dropout, Flatten, Dense, Reshape\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), input_shape=(80, 200, 1)),  # conv2d_6\n",
    "    Activation('relu'),                            # activation_9\n",
    "    BatchNormalization(),                          # batch_normalization_7\n",
    "    MaxPooling2D((2, 2)),                          # max_pooling2d_4\n",
    "    Dropout(0.25),                                 # dropout_6\n",
    "    \n",
    "    Conv2D(32, (3, 3)),                            # conv2d_7\n",
    "    Activation('relu'),                            # activation_10\n",
    "    BatchNormalization(),                          # batch_normalization_8\n",
    "    Conv2D(32, (3, 3)),                            # conv2d_8\n",
    "    Activation('relu'),                            # activation_11\n",
    "    BatchNormalization(),                          # batch_normalization_9\n",
    "    MaxPooling2D((2, 2)),                          # max_pooling2d_5\n",
    "    Dropout(0.25),                                 # dropout_7\n",
    "    \n",
    "    Conv2D(64, (3, 3)),                            # conv2d_9\n",
    "    Activation('relu'),                            # activation_12\n",
    "    BatchNormalization(),                          # batch_normalization_10\n",
    "    MaxPooling2D((2, 2)),                          # max_pooling2d_6\n",
    "    Dropout(0.25),                                 # dropout_8\n",
    "    \n",
    "    Conv2D(64, (1, 1)),                            # conv2d_10 (changed kernel size to (1, 1))\n",
    "    Activation('relu'),                            # activation_13\n",
    "    BatchNormalization(),                          # batch_normalization_11\n",
    "    MaxPooling2D((2, 2)),                          # max_pooling2d_7\n",
    "    Dropout(0.25),                                 # dropout_9\n",
    "    \n",
    "    Conv2D(128, (1, 1)),                           # conv2d_11 (changed kernel size to (1, 1))\n",
    "    Activation('relu'),                            # activation_14\n",
    "    Dropout(0.25),                                 # dropout_10\n",
    "    \n",
    "    Flatten(),                                     # flatten_1\n",
    "    Dense(2048),                                   # dense_4\n",
    "    Activation('relu'),                            # activation_15\n",
    "    BatchNormalization(),                          # batch_normalization_12\n",
    "    Dropout(0.5),                                  # dropout_10\n",
    "    \n",
    "    Dense(1024),                                   # dense_5\n",
    "    Activation('relu'),                            # activation_16\n",
    "    BatchNormalization(),                          # batch_normalization_13\n",
    "    Dropout(0.5),                                  # dropout_11\n",
    "    \n",
    "    Dense(216),                                    # dense_6\n",
    "    Reshape((6, 36)),                              # reshape_1\n",
    "    Dense(36),                                     # dense_7\n",
    "    Activation('softmax')                          # activation_17\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2dad3fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "43/43 [==============================] - 25s 418ms/step - loss: 4.6993 - accuracy: 0.0354 - val_loss: 4.0830 - val_accuracy: 0.0222\n",
      "Epoch 2/50\n",
      "43/43 [==============================] - 17s 387ms/step - loss: 3.8814 - accuracy: 0.0878 - val_loss: 6.2740 - val_accuracy: 0.0289\n",
      "Epoch 3/50\n",
      "43/43 [==============================] - 17s 386ms/step - loss: 3.0629 - accuracy: 0.1653 - val_loss: 9.2854 - val_accuracy: 0.0333\n",
      "Epoch 4/50\n",
      "43/43 [==============================] - 16s 383ms/step - loss: 2.4198 - accuracy: 0.2647 - val_loss: 11.0711 - val_accuracy: 0.0322\n",
      "Epoch 5/50\n",
      "43/43 [==============================] - 17s 384ms/step - loss: 2.0242 - accuracy: 0.3426 - val_loss: 11.8784 - val_accuracy: 0.0311\n",
      "Epoch 6/50\n",
      "43/43 [==============================] - 16s 383ms/step - loss: 1.7512 - accuracy: 0.4191 - val_loss: 13.0552 - val_accuracy: 0.0233\n",
      "Epoch 7/50\n",
      "43/43 [==============================] - 17s 385ms/step - loss: 1.5341 - accuracy: 0.4705 - val_loss: 13.2906 - val_accuracy: 0.0300\n",
      "Epoch 8/50\n",
      "43/43 [==============================] - 16s 384ms/step - loss: 1.3560 - accuracy: 0.5343 - val_loss: 14.0586 - val_accuracy: 0.0333\n",
      "Epoch 9/50\n",
      "43/43 [==============================] - 16s 369ms/step - loss: 1.2127 - accuracy: 0.5867 - val_loss: 13.2841 - val_accuracy: 0.0344\n",
      "Epoch 10/50\n",
      "43/43 [==============================] - 16s 374ms/step - loss: 1.1006 - accuracy: 0.6221 - val_loss: 11.8010 - val_accuracy: 0.0378\n",
      "Epoch 11/50\n",
      "43/43 [==============================] - 16s 382ms/step - loss: 0.9812 - accuracy: 0.6651 - val_loss: 9.0226 - val_accuracy: 0.0411\n",
      "Epoch 12/50\n",
      "43/43 [==============================] - 17s 385ms/step - loss: 0.8858 - accuracy: 0.6941 - val_loss: 7.2141 - val_accuracy: 0.0444\n",
      "Epoch 13/50\n",
      "43/43 [==============================] - 17s 385ms/step - loss: 0.8005 - accuracy: 0.7237 - val_loss: 4.4255 - val_accuracy: 0.0867\n",
      "Epoch 14/50\n",
      "43/43 [==============================] - 16s 383ms/step - loss: 0.7165 - accuracy: 0.7542 - val_loss: 2.9066 - val_accuracy: 0.1989\n",
      "Epoch 15/50\n",
      "43/43 [==============================] - 16s 384ms/step - loss: 0.6733 - accuracy: 0.7607 - val_loss: 1.5429 - val_accuracy: 0.4633\n",
      "Epoch 16/50\n",
      "43/43 [==============================] - 17s 385ms/step - loss: 0.6146 - accuracy: 0.7852 - val_loss: 1.1059 - val_accuracy: 0.5956\n",
      "Epoch 17/50\n",
      "43/43 [==============================] - 17s 387ms/step - loss: 0.5866 - accuracy: 0.7941 - val_loss: 0.7535 - val_accuracy: 0.6978\n",
      "Epoch 18/50\n",
      "43/43 [==============================] - 17s 387ms/step - loss: 0.5393 - accuracy: 0.8178 - val_loss: 0.5881 - val_accuracy: 0.7800\n",
      "Epoch 19/50\n",
      "43/43 [==============================] - 16s 382ms/step - loss: 0.5149 - accuracy: 0.8233 - val_loss: 0.4735 - val_accuracy: 0.8356\n",
      "Epoch 20/50\n",
      "43/43 [==============================] - 17s 385ms/step - loss: 0.4683 - accuracy: 0.8456 - val_loss: 0.4275 - val_accuracy: 0.8489\n",
      "Epoch 21/50\n",
      "43/43 [==============================] - 16s 383ms/step - loss: 0.4541 - accuracy: 0.8417 - val_loss: 0.3954 - val_accuracy: 0.8556\n",
      "Epoch 22/50\n",
      "43/43 [==============================] - 16s 384ms/step - loss: 0.4188 - accuracy: 0.8556 - val_loss: 0.3318 - val_accuracy: 0.8789\n",
      "Epoch 23/50\n",
      "43/43 [==============================] - 16s 384ms/step - loss: 0.4018 - accuracy: 0.8586 - val_loss: 0.3387 - val_accuracy: 0.8833\n",
      "Epoch 24/50\n",
      "43/43 [==============================] - 16s 376ms/step - loss: 0.3774 - accuracy: 0.8730 - val_loss: 0.3158 - val_accuracy: 0.8833\n",
      "Epoch 25/50\n",
      "43/43 [==============================] - 17s 384ms/step - loss: 0.3372 - accuracy: 0.8863 - val_loss: 0.3149 - val_accuracy: 0.9033\n",
      "Epoch 26/50\n",
      "43/43 [==============================] - 17s 386ms/step - loss: 0.3119 - accuracy: 0.8931 - val_loss: 0.2984 - val_accuracy: 0.8900\n",
      "Epoch 27/50\n",
      "43/43 [==============================] - 16s 382ms/step - loss: 0.3156 - accuracy: 0.8931 - val_loss: 0.2429 - val_accuracy: 0.9144\n",
      "Epoch 28/50\n",
      "43/43 [==============================] - 16s 384ms/step - loss: 0.2938 - accuracy: 0.8998 - val_loss: 0.2252 - val_accuracy: 0.9256\n",
      "Epoch 29/50\n",
      "43/43 [==============================] - 17s 385ms/step - loss: 0.3151 - accuracy: 0.8919 - val_loss: 0.2466 - val_accuracy: 0.9178\n",
      "Epoch 30/50\n",
      "43/43 [==============================] - 16s 383ms/step - loss: 0.2848 - accuracy: 0.9023 - val_loss: 0.2193 - val_accuracy: 0.9222\n",
      "Epoch 31/50\n",
      "43/43 [==============================] - 16s 382ms/step - loss: 0.2791 - accuracy: 0.9070 - val_loss: 0.2050 - val_accuracy: 0.9322\n",
      "Epoch 32/50\n",
      "43/43 [==============================] - 17s 385ms/step - loss: 0.2569 - accuracy: 0.9090 - val_loss: 0.1942 - val_accuracy: 0.9433\n",
      "Epoch 33/50\n",
      "43/43 [==============================] - 16s 384ms/step - loss: 0.2344 - accuracy: 0.9185 - val_loss: 0.2102 - val_accuracy: 0.9300\n",
      "Epoch 34/50\n",
      "43/43 [==============================] - 17s 385ms/step - loss: 0.2275 - accuracy: 0.9225 - val_loss: 0.1926 - val_accuracy: 0.9356\n",
      "Epoch 35/50\n",
      "43/43 [==============================] - 16s 382ms/step - loss: 0.2129 - accuracy: 0.9264 - val_loss: 0.1797 - val_accuracy: 0.9500\n",
      "Epoch 36/50\n",
      "43/43 [==============================] - 16s 382ms/step - loss: 0.2169 - accuracy: 0.9237 - val_loss: 0.1890 - val_accuracy: 0.9378\n",
      "Epoch 37/50\n",
      "43/43 [==============================] - 17s 384ms/step - loss: 0.2232 - accuracy: 0.9277 - val_loss: 0.1784 - val_accuracy: 0.9489\n",
      "Epoch 38/50\n",
      "43/43 [==============================] - 16s 383ms/step - loss: 0.2156 - accuracy: 0.9252 - val_loss: 0.1742 - val_accuracy: 0.9456\n",
      "Epoch 39/50\n",
      "43/43 [==============================] - 16s 381ms/step - loss: 0.2176 - accuracy: 0.9277 - val_loss: 0.1675 - val_accuracy: 0.9522\n",
      "Epoch 40/50\n",
      "43/43 [==============================] - 17s 385ms/step - loss: 0.1961 - accuracy: 0.9321 - val_loss: 0.1507 - val_accuracy: 0.9611\n",
      "Epoch 41/50\n",
      "43/43 [==============================] - 16s 384ms/step - loss: 0.1823 - accuracy: 0.9401 - val_loss: 0.1468 - val_accuracy: 0.9633\n",
      "Epoch 42/50\n",
      "43/43 [==============================] - 16s 382ms/step - loss: 0.1792 - accuracy: 0.9381 - val_loss: 0.1407 - val_accuracy: 0.9589\n",
      "Epoch 43/50\n",
      "43/43 [==============================] - 16s 384ms/step - loss: 0.1804 - accuracy: 0.9377 - val_loss: 0.1467 - val_accuracy: 0.9589\n",
      "Epoch 44/50\n",
      "43/43 [==============================] - 16s 383ms/step - loss: 0.1692 - accuracy: 0.9407 - val_loss: 0.1384 - val_accuracy: 0.9567\n",
      "Epoch 45/50\n",
      "43/43 [==============================] - 16s 377ms/step - loss: 0.1814 - accuracy: 0.9384 - val_loss: 0.1431 - val_accuracy: 0.9533\n",
      "Epoch 46/50\n",
      "43/43 [==============================] - 17s 386ms/step - loss: 0.1649 - accuracy: 0.9432 - val_loss: 0.1306 - val_accuracy: 0.9644\n",
      "Epoch 47/50\n",
      "43/43 [==============================] - 16s 383ms/step - loss: 0.1661 - accuracy: 0.9409 - val_loss: 0.1282 - val_accuracy: 0.9633\n",
      "Epoch 48/50\n",
      "43/43 [==============================] - 16s 378ms/step - loss: 0.1604 - accuracy: 0.9432 - val_loss: 0.1274 - val_accuracy: 0.9611\n",
      "Epoch 49/50\n",
      "43/43 [==============================] - 16s 382ms/step - loss: 0.1467 - accuracy: 0.9488 - val_loss: 0.1199 - val_accuracy: 0.9678\n",
      "Epoch 50/50\n",
      "43/43 [==============================] - 16s 382ms/step - loss: 0.1501 - accuracy: 0.9470 - val_loss: 0.1227 - val_accuracy: 0.9656\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels, epochs=50, \n",
    "                    validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929b26f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bc33f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(r'C:\\Users\\DELL\\Desktop\\Python\\Code\\Capture\\Captcha_model3.1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e9c5c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "1/1 [==============================] - 0s 353ms/step\n",
      "Predicted Captcha: fwam4g\n"
     ]
    }
   ],
   "source": [
    "# This is for prediction of captcha using trained model\n",
    "import numpy as np\n",
    "from PIL import Image,ImageFilter\n",
    "from tensorflow.keras.models import load_model\n",
    "pass_factor = 179\n",
    "\n",
    "def prepare_image(img):\n",
    "    \"\"\"Transform image to greyscale and blur it\"\"\"\n",
    "    img = img.filter(ImageFilter.SMOOTH_MORE)\n",
    "    img = img.filter(ImageFilter.SMOOTH_MORE)\n",
    "    if 'L' != img.mode:\n",
    "        img = img.convert('L')\n",
    "    return img\n",
    "\n",
    "def remove_noise_by_pixel(img, column, line, pass_factor):\n",
    "    if img.getpixel((column, line)) < pass_factor:\n",
    "        return (0)\n",
    "    return (255)\n",
    "\n",
    "def remove_noise(img, pass_factor):\n",
    "    for column in range(img.size[0]):\n",
    "        for line in range(img.size[1]):\n",
    "            value = remove_noise_by_pixel(img, column, line, pass_factor)\n",
    "            img.putpixel((column, line), value)\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Load the image and preprocess it\n",
    "    image = Image.open(image_path).filter(ImageFilter.SHARPEN).convert(\"L\")  # Convert to grayscale\n",
    "    image = prepare_image(image)\n",
    "    image = remove_noise(image,pass_factor)\n",
    "    image = np.array(image) / 255.0  # Normalize pixel values to [0, 1]\n",
    "    image = np.expand_dims(image, axis=-1)  # Add a channel dimension\n",
    "    image = np.expand_dims(image, axis=0)  # Add a batch dimension\n",
    "    return image\n",
    "\n",
    "def decode_captcha(predicted_captcha):\n",
    "    # Implement your logic to decode the predicted captcha\n",
    "    # This depends on how the labels were encoded during training\n",
    "    # You might need to reverse the one-hot encoding and map it to characters\n",
    "    # For example, if labels were one-hot encoded using characters '0123456789abcdefghijklmnopqrstuvwxyz',\n",
    "    # you can reverse the one-hot encoding and map it back to characters using argmax function\n",
    "    \n",
    "    # Placeholder logic, modify according to your label encoding\n",
    "    characters = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "    decoded_captcha = ''\n",
    "    for prediction in predicted_captcha[0]:\n",
    "        index = np.argmax(prediction)\n",
    "        decoded_captcha += characters[index]\n",
    "    \n",
    "    return decoded_captcha\n",
    "\n",
    "def predict_captcha(image_path, model):\n",
    "    # Preprocess the image\n",
    "    image = preprocess_image(image_path)\n",
    "    \n",
    "    # Load the trained model\n",
    "#     model = load_model(model_path)\n",
    "    \n",
    "    # Predict the captcha\n",
    "    predicted_captcha = model.predict(image)\n",
    "    \n",
    "    # Convert the predicted captcha from one-hot encoding to a string\n",
    "    captcha_string = decode_captcha(predicted_captcha)\n",
    "    \n",
    "    return captcha_string\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "image_path = r'C:\\Users\\DELL\\Desktop\\Python\\Code\\Capture\\iwum4g.png'  # Replace with the path to your captcha image\n",
    "model_path = r'C:\\Users\\DELL\\Desktop\\Python\\Captcha_model3.3.h5'   # Replace with the path to your trained model\n",
    "model = load_model(model_path)\n",
    "predicted_captcha = predict_captcha(image_path, model)\n",
    "print(\"Predicted Captcha:\", predicted_captcha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
